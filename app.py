import streamlit as st
import pickle
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

# -------------------------
# NLTK Downloads
# -------------------------
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# -------------------------
# Load model & vectorizer
# -------------------------
with open('logreg_model.pkl', 'rb') as f:
    model = pickle.load(f)

with open('tfidf_vectorizer.pkl', 'rb') as f:
    tfidvect = pickle.load(f)

# -------------------------
# Streamlit Page Config
# -------------------------
st.set_page_config(
    page_title="Fake News Detector",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# -------------------------
# Sidebar (User Guide)
# -------------------------
st.sidebar.title("ğŸ“˜ How to Use")
st.sidebar.markdown("""
Welcome to the **Fake News Detection App**!  
Follow these steps:

1ï¸âƒ£ **Enter the news source**  
Examples: *Reuters, BBC, CNN...*

2ï¸âƒ£ **Paste the full news text**  
The more text you provide, the more accurate the prediction becomes.

3ï¸âƒ£ **Click 'Predict'**  
You'll instantly know if the news is **REAL ğŸ“°** or **FAKE âš ï¸**.

---

### ğŸ’¡ Tips for best accuracy:
- Avoid extremely short texts.
- Provide the full article when possible.
- Mention a trusted source if available.

Enjoy using the app! ğŸ˜„
""")

# -------------------------
# Main Title
# -------------------------
st.markdown("<h1 style='text-align: center; color:#4A90E2;'>ğŸ” Fake News Detection</h1>", unsafe_allow_html=True)
st.markdown("<h4 style='text-align: center; color: gray;'>Smart AI system to help you detect misinformation in seconds</h4>", unsafe_allow_html=True)

st.write("")  # Space

# -------------------------
# Input Section
# -------------------------
st.markdown("### ğŸ·ï¸ News Source")
source = st.text_input(
    label="",
    placeholder="Examples: Reuters, BBC, New York Times...",
)

st.markdown("### âœï¸ News Text")
text = st.text_area(
    label="",
    placeholder="Paste the news text here...",
    height=230
)

# ----------------------------------------------------------
# (ØªÙ… Ø­Ø°Ù Ø¬Ø²Ø¡ Ø¹Ø¯Ù‘ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨Ø§Ù„ÙƒØ§Ù…Ù„)
# ----------------------------------------------------------

# -------------------------
# Preprocessing function
# -------------------------
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower().strip()
    text = re.sub(r'http\S+|www\S+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

# -------------------------
# Prediction function
# -------------------------
def predict(text, source=""):
    combined_text = f"{source} {text}" if source else text
    processed = preprocess_text(combined_text)
    vectorized = tfidvect.transform([processed])
    prediction = model.predict(vectorized)[0]  # 1=FAKE, 0=REAL
    return 'FAKE âš ï¸' if prediction == 1 else 'REAL ğŸ“°'

# -------------------------
# Predict Button
# -------------------------
st.write("")
predict_btn = st.button("ğŸ”® Predict Now")

if predict_btn:
    if not text.strip():
        st.warning("âš ï¸ Please enter some news text first!")
    else:
        result = predict(text, source)

        if "FAKE" in result:
            st.error(f"### ğŸš¨ Prediction: {result}")
        else:
            st.success(f"### âœ… Prediction: {result}")
